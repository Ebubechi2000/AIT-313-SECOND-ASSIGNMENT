QUESTION 1
COMPARE AND CONTRAST AUGUMENTED INTELLIGENCE AND ARTIFICIAL INTELLIGENCE
a. AI aims to create autonomous systems that can perform tasks independently, simulating human intelligence. 

b. AI focuses on creating autonomous systems that simulate human intelligence WHILE Augmented Intelligence aims to enhance human capabilities by leveraging machine intelligence. Both technologies have the potential to transform industries and revolutionize the way we work

c. AI relies heavily on large datasets to learn and improve, WHILE Augmented Intelligence can function with smaller datasets and more human input.

d. AI models can be difficult to interpret and explain, WHILE Augmented Intelligence systems are designed to provide transparent and explainable recommendations.

e. AI systems often require human oversight to correct errors or biases, WHILE Augmented Intelligence systems are designed to collaborate with humans and learn from their feedback.

f. AI systems can be domain agnostic, WHILE Augmented Intelligence systems often require domain-specific knowledge and expertise.

5 AI systems can be designed to scale horizontally,WHILE  Augmented Intelligence systems often require more vertical scaling, with a focus on human-machine collaboration.

QUESTION 2
History of AI from 1940 till date

In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research.[1] The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.

The term first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the "American Association of Artificial Intelligence").[2] Roger Schank and Marvin Minsky—two leading AI researchers who experienced the "winter" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. They described a chain reaction, similar to a "nuclear winter", that would begin with pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research.[2] Three years later the billion-dollar AI industry began to collapse.

There were two major "winters" approximately 1974–1980 and 1987–2000,[3] and several smaller episodes, including the following:

1966: failure of machine translation
1969: criticism of perceptrons (early, single-layer artificial neural networks)
1971–75: DARPA's frustration with the Speech Understanding Research program at Carnegie Mellon University
1973: large decrease in AI research in the United Kingdom in response to the Lighthill report
1973–74: DARPA's cutbacks to academic AI research in general
1987: collapse of the LISP machine market
1988: cancellation of new spending on AI by the Strategic Computing Initiative
1990s: many expert systems were abandoned
1990s: end of the Fifth Generation computer project's original goals
Enthusiasm and optimism about AI has generally increased since its low point in the early 1990s. Beginning about 2012, interest in artificial intelligence (and especially the sub-field of machine learning) from the research and corporate communities led to a dramatic increase in funding and investment, leading to the current (as of 2025) AI boom.
